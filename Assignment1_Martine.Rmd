---
title: "Assignment1"
author: "Martine Lind Jensen"
date: "2024-02-08"
output: pdf_document
---
```{r loading libraries}
pacman::p_load("tidyverse", "hesim")
```

Riccardos code to for random agent to let our models play against 
```{r random agent from Riccardo to build models against}

rate <- 0.5

# now as a function
RandomAgent_f <- function(input, rate){
  n <- length(input)
  choice <- rbinom(n, 1, rate)
  return(choice)
}
```

Reinforcement learning model 
```{r RL agent playing against random agent}
#Reinforcement learning agent 

#Making a function for expected value based on RL 
  #takes in feedback(reward), learning rate(alpha), and the previous expected value to update a new expected value for the next trial

ev <- function(prevChoice, feedback, alpha, prev_ev){
  
  if (prevChoice == 1){ev_update = prev_ev + alpha * (feedback - prev_ev)} #Here we are only indicating that we want values for the same hand during all trials, which means that our ev_update only updates expected values for hand 1 since the expected values for the hands are orthogonal 
  else (ev_update = 1 - (prev_ev + alpha * (feedback - prev_ev)))

  #Implemtenting softmax (theta set to 1)
  probability = exp(ev_update)/(exp(ev_update)+ exp(1-ev_update)) #1-ev_update means that we are summing from both "hands"
  
  choice = rbinom(1, 1, probability)
  
  outcome = c(choice, ev_update)
  
  return(outcome)
}

#Trying the function
#try <- ev(1, 1, 0.3, 0.51)


#Players choices 
Self <- rep(NA, trials)
Other <- rep(NA, trials)

#Other lists to populate
ev_list <- rep(NA, trials)
feedback_list <- rep(NA, trials)

Self[1] <- rbinom(1, 1, 0.5) #Randomizing first choice for self 

Other[1] <- rbinom(1, 1, 0.5) 

ev_list[1] <- 0.5

#Setting learning rate 
alpha = 0.3 #We try learning rates (0.3, 0.5, 1)

for (t in 2:trials){
  
  # get feedback
  if (Self[t-1] == Other[t-1]){ Feedback = 1} 
  else {Feedback = 0}
  
  outcome <- ev(Self[t-1], Feedback, alpha, ev_list[t-1])
  
  # register feedback
  feedback_list[t] <- Feedback
  
  #Register choices
  Self[t] <- outcome[1]
  
  #Register expected value
  ev_list[t] <- outcome[2]
  
  Other[t] <- RandomAgent_f(trials, rate)
}

df_RLvsRandom <- tibble(Self, Other, trial = seq(trials), feedback_list)
```

```{r plotting for RL model}
#plotting
df <- tibble(Self, Other, trial = seq(trials),Feedback = as.numeric(Self == Other))
ggplot(df) + theme_classic() +
  geom_line(color = "red", aes(trial, Self)) +
  geom_line(color = "blue", aes(trial, Other))

df$cumulativerateSelf <- cumsum(df$Feedback) / seq_along(df$Feedback)
df$cumulativerateOther <- cumsum(1 - df$Feedback) / seq_along(df$Feedback)

ggplot(df) + theme_classic() +
  geom_line(color = "red", aes(trial, cumulativerateSelf)) +
  geom_line(color = "blue", aes(trial, cumulativerateOther))
```

Asymmetrical win stay loose shift model with different probabilities for wins and losses.
```{r Asym WSLS agent}
# Building function for asymmetric win stay loose shift agent by including probabilities for winning and loosing

AsymWSLSAgent_f <- function(prevChoice, Feedback, winProb, lossProb){
  
  if (Feedback == 1){
    
    randomness <- rbinom(1, 1, winProb)
    # stay with choice agent would initially have made
    if (randomness == 1){
      choice = prevChoice 
    # randomly shift
    } else if (randomness == 0){
      choice = 1-prevChoice
    }
      
  } else if (Feedback == 0){
    
    randomness <- rbinom(1, 1, lossProb)
    # stay with choice agent would initially have made
    if (randomness == 1){
      choice = 1-prevChoice
    # randomly shift
    } else if (randomness == 0){
      choice = prevChoice
    }
  }
  
  return(choice)
}

# empty vectors for agents
Self2 <- rep(NA, trials)
Other2 <- rep(NA, trials)
Feedback2 <- rep(NA, trials)

# other agent's choices
for (t in seq(trials)){
  Other2[t] <- RandomAgent_f(rate)
}

# first choice is random...
Self[1] <- RandomAgent_f(0.5)

# all other choices
for (i in 2:trials){
  
  # get feedback
  if (Self[i-1] == Other[i-1]){
    Feedback = 1
  } else {Feedback = 0}
  
  # register feedback
  Feedback2[i] <- Feedback
  
  # make decision
  Self[i] <- AsymWSLSAgent_f(Self[i-1], 
                         Feedback,
                         0.9,
                         0.7)
}

# turn into tibble
d3 <- tibble(Self,
             Other,
             trial = seq(trials),
             Feedback = Feedback2)

d3
```

Setting the RL model up against asymWSLS model. 
```{r RL against asymWSLS}

#Players choices 
Self <- rep(NA, trials)
Other <- rep(NA, trials)

#Other lists to populate
ev_list <- rep(NA, trials)
feedbackRL_list <- rep(NA, trials)

Self[1] <- rbinom(1, 1, 0.5) #Randomizing first choice for self 

Other[1] <- rbinom(1, 1, 0.5) 

ev_list[1] <- 0.5

#Setting learning rate 
alpha = 1 #We try learning rates (0.3, 0.5, 1)

for (t in 2:trials){
  
  # get feedback
  if (Self[t-1] == Other[t-1]){ Feedback = 1} 
  else {Feedback = 0}
  
  outcome <- ev(Self[t-1], Feedback, alpha, ev_list[t-1])
  
  # register feedback
  feedbackRL_list[t] <- Feedback
  
  #Register choices
  Self[t] <- outcome[1]
  
  #Register expected value
  ev_list[t] <- outcome[2]
  
  #Opposite feedback for other 
  if (Self[t-1] == Other[t-1]){ Feedback2 = 0} 
  else {Feedback2 = 1}
  
  Other[t] <- AsymWSLSAgent_f(Other[t - 1], Feedback2, 0.9, 0.7)
}

#Create a tibble for each learning rate (d.3, d.5, d.1)
d.1 <- tibble(Self,
             Other,
             trial = seq(trials),
             FeedbackRL = as.numeric(Self == Other)) %>% mutate(
               cumulativerateSelf = cumsum(d.1$FeedbackRL) / seq_along(d.1$FeedbackRL), 
               cumulativerateOther = cumsum(1 - d.1$FeedbackRL) / seq_along(d.1$FeedbackRL)
             )
```

```{r}
#plotting the three different learning rates 

ggplot(d.1, aes(trials, cumulative)) + theme_minimal() +
  geom_line(aes(trial, cumulativerateSelf), color = "darkred") +
  geom_line(aes(trial, cumulativerateOther), color = "darkolivegreen4")
  
ggplot(d.3, aes(trials, cumulative)) + theme_minimal() +
  geom_line(aes(trial, cumulativerateSelf), color = "darkred") +
  geom_line(aes(trial, cumulativerateOther), color = "darkolivegreen4")

ggplot(d.5, aes(trials, cumulative)) + theme_minimal() +
  geom_line(aes(trial, cumulativerateSelf), color = "darkred") +
  geom_line(aes(trial, cumulativerateOther), color = "darkolivegreen4")
```

Scaling up with different learning rates 
```{r}
trials = 120
agents = 100

df1 <- data.frame()

# WSLS vs agents with varying rates

for (alpha in seq(from = 0.1, to = 1, by = 0.1)) {
  
  for (agent in seq(agents)) {
      #Players choices 
    Self <- rep(NA, trials)
    Other <- rep(NA, trials)
    
    #Other lists to populate
    ev_list <- rep(NA, trials)
    feedback_list <- rep(NA, trials)
    
    Self[1] <- rbinom(1, 1, 0.5) #Randomizing first choice for self 
    
    Other[1] <- rbinom(1, 1, 0.5) 
    
    ev_list[1] <- 0.5 
      
    
    for (t in 2:trials){
    
      #get feedback
      if (Self[t-1] == Other[t-1]){ Feedback = 1} 
      else {Feedback = 0}
      
      outcome <- ev(Self[t-1], Feedback, alpha, ev_list[t-1])
      
      # register feedback
      feedback_list[t] <- Feedback
      
      #Register choices
      Self[t] <- outcome[1]
      
      #Register expected value
      ev_list[t] <- outcome[2]
      
      #Opposite feedback for other 
      if (Self[t-1] == Other[t-1]){ Feedback2 = 0} 
      else {Feedback2 = 1}
      
      Other[t] <- AsymWSLSAgent_f(Other[t - 1], Feedback2, 0.9, 0.7)
    }
    
  }  
    
    temp <- tibble(Self, Other, trial = seq(trials), feedback_list, agent, alpha)
    
    if (agent == 1 ) {df1 <- temp} else {df1 <- bind_rows(df1, temp)}
}
```

Plotting the learning rate parameters 
```{r}
df1 %>% mutate(alpha = as.character(alpha)) %>% ggplot(aes(trial, feedback_list, group = alpha, color = alpha)) +
  geom_smooth(se = F) + theme_classic()

df1 %>% mutate(alpha = as.character(alpha)) %>% ggplot(aes(trial, Feedback_WSLS, group = alpha, color = alpha)) +
  geom_smooth(se = F) + theme_classic()
```



